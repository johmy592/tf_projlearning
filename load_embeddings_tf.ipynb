{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION=100 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = '/home/johannes/thesis_code/word_embeddings/'\n",
    "\n",
    "glove_weights_file_path = '/home/johannes/thesis_code/word_embeddings/glove.6B.100d.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers weights = []\n",
    "\n",
    "weights=[]\n",
    "with open(glove_weights_file_path,'r') as file:\n",
    "    for index,line in enumerate(file):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32)\n",
    "        word2idx[word] = index + 1\n",
    "        weights.append(word_weights)\n",
    "        if index + 1 == 40000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "UNKNOWN_TOKEN=len(weights) \n",
    "word2idx['UNK'] = UNKNOWN_TOKEN \n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "VOCAB_SIZE=weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = {}\n",
    "features['word_indices'] = nltk.word_tokenize('hello world') # ['hello', 'world']\n",
    "features['word_indices'] = [word2idx.get(word, UNKNOWN_TOKEN) for word in features['word_indices']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights_initializer = tf.constant_initializer(weights)\n",
    "embedding_weights = tf.get_variable(\n",
    "    name='embedding_weights', \n",
    "    shape=(VOCAB_SIZE, EMBEDDING_DIMENSION), \n",
    "    initializer=glove_weights_initializer,\n",
    "    trainable=False)\n",
    "#embedding = tf.nn.embedding_lookup(embedding_weights, features['word_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'embedding_weights:0' shape=(40002, 100) dtype=float32_ref>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n******************\\nDONE WITH\\nWORD EMBEDDINGS??\\n******************\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "******************\n",
    "DONE WITH\n",
    "WORD EMBEDDINGS??\n",
    "******************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "******************\n",
    "READING THE TRAINING DATA\n",
    "******************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/johannes/thesis_code/ml_experimentation/data/training/1A.english.training.data.txt\"\n",
    "train_gold_path = \"/home/johannes/thesis_code/ml_experimentation/data/training/1A.english.training.gold.txt\"\n",
    "\n",
    "data_file = open(train_data_path,'r')\n",
    "gold_file = open(train_gold_path,'r')\n",
    "\n",
    "train_data = [line.split('\\t')[0] for line in data_file]\n",
    "gold_data = [[w.strip('\\n') for w in line.split('\\t')] for line in gold_file]\n",
    "\n",
    "data_file.close()\n",
    "gold_file.close()\n",
    "assert len(train_data) == len (gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_without_embeddings(train, gold):\n",
    "    train_embeddings = []\n",
    "    gold_embeddings = []\n",
    "    for i in range(len(train)):\n",
    "        if train[i] not in word2idx:\n",
    "            continue\n",
    "        if not np.array([w in word2idx for w in gold[i]]).any():\n",
    "            continue\n",
    "        train_embeddings.append(train[i])\n",
    "        gold_embeddings += [[w for w in gold[i] if w in word2idx]]\n",
    "    assert len(train_embeddings) == len(gold_embeddings)\n",
    "    print(\"kept \", len(train_embeddings), \" words with embeddings\")\n",
    "    return train_embeddings, gold_embeddings\n",
    "        \n",
    "def create_training_dict(train, gold, add_neg=0):\n",
    "    '''\n",
    "    Takes lists of indices\n",
    "    '''\n",
    "    possible_negatives = []\n",
    "    if add_neg:\n",
    "        possible_negatives = list(word2idx.keys())\n",
    "    \n",
    "    train_dict = {}\n",
    "    train_dict[\"queries\"] = []\n",
    "    train_dict[\"candidates\"] = []\n",
    "    train_dict[\"targets\"] = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        current_query = train[i]\n",
    "        current_candidates = [h for h in gold[i]]\n",
    "        current_negatives = [word2idx[random.choice(possible_negatives)] for _ in range(add_neg*len(current_candidates))]\n",
    "        num_pos = len(current_candidates)\n",
    "        num_neg = len(current_negatives)\n",
    "        \n",
    "        train_dict[\"queries\"] += [current_query]*num_pos\n",
    "        train_dict[\"candidates\"] += current_candidates\n",
    "        train_dict[\"targets\"] += [1]*num_pos\n",
    "        if add_neg:\n",
    "            train_dict[\"queries\"] += [current_query]*num_neg\n",
    "            train_dict[\"candidates\"] += current_negatives\n",
    "            train_dict[\"targets\"] += [0]*num_neg\n",
    "    assert len(train_dict[\"queries\"]) == len(train_dict[\"candidates\"]) == len(train_dict[\"targets\"])\n",
    "    return train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept  412  words with embeddings\n",
      "[4400, 1508, 1294, 12323, 22635, 2814, 13929, 34347, 5252, 13769]\n"
     ]
    }
   ],
   "source": [
    "train_data, gold_data = remove_words_without_embeddings(train_data,gold_data)\n",
    "train_indices = [word2idx.get(word, UNKNOWN_TOKEN) for word in train_data]\n",
    "gold_indices = [[word2idx.get(word, UNKNOWN_TOKEN) for word in line] for line in gold_data]\n",
    "\n",
    "assert len(train_indices) == len(gold_indices)\n",
    "\n",
    "train_dict = create_training_dict(train_indices, gold_indices, 3)\n",
    "\n",
    "\"\"\"\n",
    "Now have a train_dict with\n",
    "\"queries\",\"candidates\" and \"targets\" (word_idx, word_idx, 1/0)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n******************\\nPERFORMING THE TRAINING\\n******************\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "******************\n",
    "PERFORMING THE TRAINING\n",
    "AND STUFF\n",
    "******************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(EMBEDDING_DIMENSION)) # mnist data image of shape 28*28=784\n",
    "h = tf.placeholder(tf.float32, shape=(EMBEDDING_DIMENSION))\n",
    "y = tf.placeholder(tf.float32) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.math.add(tf.eye(EMBEDDING_DIMENSION),tf.random.normal([EMBEDDING_DIMENSION, EMBEDDING_DIMENSION],0,0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = tf.matmul(tf.expand_dims(x,0),W)\n",
    "sim = tf.tensordot(proj, h, 1)\n",
    "pred = tf.math.sigmoid(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid_1:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.038194  -0.24487    0.72812   -0.39961    0.083172   0.043953\n",
      "  -0.39141    0.3344    -0.57545    0.087459   0.28787   -0.06731\n",
      "   0.30906   -0.26384   -0.13231   -0.20757    0.33395   -0.33848\n",
      "  -0.31743   -0.48336    0.1464    -0.37304    0.34577    0.052041\n",
      "   0.44946   -0.46971    0.02628   -0.54155   -0.15518   -0.14107\n",
      "  -0.039722   0.28277    0.14393    0.23464   -0.31021    0.086173\n",
      "   0.20397    0.52624    0.17164   -0.082378  -0.71787   -0.41531\n",
      "   0.20335   -0.12763    0.41367    0.55187    0.57908   -0.33477\n",
      "  -0.36559   -0.54857   -0.062892   0.26584    0.30205    0.99775\n",
      "  -0.80481   -3.0243     0.01254   -0.36942    2.2167     0.72201\n",
      "  -0.24978    0.92136    0.034514   0.46745    1.1079    -0.19358\n",
      "  -0.074575   0.23353   -0.052062  -0.22044    0.057162  -0.15806\n",
      "  -0.30798   -0.41625    0.37972    0.15006   -0.53212   -0.2055\n",
      "  -1.2526     0.071624   0.70565    0.49744   -0.42063    0.26148\n",
      "  -1.538     -0.30223   -0.073438  -0.28312    0.37104   -0.25217\n",
      "   0.016215  -0.017099  -0.38984    0.87424   -0.72569   -0.51058\n",
      "  -0.52028   -0.1459     0.8278     0.27062  ]\n",
      " [-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "   0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "   0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      "  -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "   0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      "  -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "   0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "   0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      "  -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      "  -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      "  -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "   0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      "  -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      "  -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      "  -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "   0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      "  -0.35111   -0.83155    0.45293    0.082577 ]\n",
      " [-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "   0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "   0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      "  -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "   0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      "  -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "   0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      "  -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      "  -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      "  -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      "  -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "   0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "   0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      "  -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      "  -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      "  -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      "  -0.39242   -0.23394    0.47298   -0.028803 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    #x = tf.get_variable('x', shape=[VOCAB_SIZE, EMBEDDING_DIMENSION], initializer=glove_weights_initializer)\n",
    "    #x.initializer.run()\n",
    "    embedding = tf.nn.embedding_lookup(embedding_weights, [1,2,3])\n",
    "    print(embedding.eval())\n",
    "    # Embeddings\n",
    "    #sess.run(embedding_init, feed_dict={embedding_placeholder: list(em.items())})\n",
    "\n",
    "    # Training cycle\n",
    "    \"\"\"\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
